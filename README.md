# RAG Chatbot with Local Llama 3 and Research Paper Retrieval

This repository demonstrates a **Retrieval-Augmented Generation (RAG) chatbot** that can answer questions using the content of your own research papers (PDFs). It uses:

- **LangChain** for document loading, chunking, and vector search
- **ChromaDB** as the vector database
- **Sentence Transformers** for embedding text
- **Ollama** to run a local Llama 3 model for answer generation
- **Streamlit** for a simple web interface
- **GPU acceleration** (CUDA for NVIDIA, MPS for Apple Silicon, or CPU fallback)

---

## Features

- **Ingest PDFs:** Place your research papers in the `pdfs/` folder and run the ingestion script to index them.
- **Semantic Search:** Uses vector embeddings to find the most relevant chunks from your documents.
- **Local LLM Answers:** Answers are generated by a local Llama 3 model (8B) via Ollama—no cloud API needed.
- **Web UI:** Ask questions interactively in your browser with Streamlit.
- **Automatic GPU/CPU Selection:** Uses CUDA (NVIDIA GPU) if available, then MPS (Apple Silicon), then CPU.

---

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/ArisKaralis/ResearchPaperRAG
cd RAG
```

### 2. Create and Activate the Environment

Create the environment using the provided `environment.yml`:

```bash
conda env create -f environment.yml
conda activate rag-env
```

- **NVIDIA GPU users:** The environment includes `pytorch-cuda=12.1` for CUDA support.  
  If you do **not** have an NVIDIA GPU, comment out or remove the `pytorch-cuda=12.1` line in `environment.yml` before creating the environment.
- **Apple Silicon (M1/M2/M3):** The conda-forge PyTorch build will use MPS automatically if available.

### 3. Install and Set Up Ollama

- [Download and install Ollama](https://ollama.com/download) for your OS.
- Pull the small Llama 3 model:

  ```bash
  ollama pull llama3:8b
  ```

- Make sure Ollama is running:

  ```bash
  ollama serve &
  ```

### 4. Ingest Your PDFs

Place your PDF files in the `pdfs/` folder.

Run the ingestion script:

```bash
python ingest_pdfs.py
```

This will extract, chunk, embed, and store your documents in ChromaDB.

### 5. Run the Streamlit Chatbot

```bash
streamlit run app.py
```

Open the provided local URL in your browser to chat with your research papers!

---

## File Overview

- `ingest_pdfs.py` — Script to load, chunk, embed, and store PDFs in ChromaDB.
- `app.py` — Streamlit web app for interactive Q&A.
- `rag_chat.py` — (Optional) Terminal-based chatbot.
- `pdfs/` — Place your research papers here.
- `chroma_db/` — Vector database files (auto-generated).

---

## Customization

- **Change the LLM:** Edit `app.py` to use a different Ollama model if desired.
- **Chunk Size:** Adjust chunking parameters in `ingest_pdfs.py` for your use case.
- **Add Metadata:** Enhance retrieval with document titles, page numbers, etc.

---

## License

MIT License
